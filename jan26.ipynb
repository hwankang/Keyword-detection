{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jan26.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKMvVEJ9rsTcrqsHDU9rB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwankang/Keyword-detection/blob/master/jan26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEAGtl_e0ANs"
      },
      "source": [
        "!wget http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\r\n",
        "!tar -zxvf speech_commands_v0.02.tar.gz\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmrEEPY_0Djx"
      },
      "source": [
        "\r\n",
        "!pip install python_speech_features\r\n",
        "from os import listdir\r\n",
        "from os.path import isdir, join\r\n",
        "import librosa\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import python_speech_features\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xYjV1Ju03JI"
      },
      "source": [
        "dataset_path='/content'\r\n",
        "for name in listdir(dataset_path):\r\n",
        "    if isdir(join(dataset_path, name)):\r\n",
        "        print(name)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuudIyQa1Bg5"
      },
      "source": [
        "all_targets=[name for name in listdir(dataset_path)  if isdir(join(dataset_path,name))]\r\n",
        "print(all_targets)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPCXTseP09uq"
      },
      "source": [
        "all_targets.remove('_background_noise_')\r\n",
        "all_targets.remove('.config')\r\n",
        "print(all_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFbyral71OTL"
      },
      "source": [
        "all_targets.remove('sample_data')\r\n",
        "print(all_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmom2fDT1ZVw"
      },
      "source": [
        "# see how many files are in each \r\n",
        "num_samples = 0\r\n",
        "for target in all_targets:\r\n",
        "    print(len(listdir(join(dataset_path,target))))\r\n",
        "    #폴더의 내용 \r\n",
        "    num_samples += len(listdir(join(dataset_path, target)))\r\n",
        "print('Total samples=', num_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwhb65pF1nBW"
      },
      "source": [
        "target_list = all_targets\r\n",
        "feature_sets_file = 'all_targets_mfcc_sets_0123.npz'\r\n",
        "#perc_keep_samples =0.1\r\n",
        "#val_ratio = 0.1\r\n",
        "#test_ratio = 0.1\r\n",
        "perc_keep_samples =0.1\r\n",
        "val_ratio = 0.1\r\n",
        "test_ratio = 0.1\r\n",
        "#sample_rate = 8000\r\n",
        "sample_rate = 16000\r\n",
        "#num_mfcc = 40\r\n",
        "num_mfcc = 20\r\n",
        "#num_mfcc = 16\r\n",
        "#len_mfcc = 16\r\n",
        "len_mfcc = 49"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFHwsqI51uv0"
      },
      "source": [
        "filenames=[]\r\n",
        "y=[]\r\n",
        "for index, target in enumerate(target_list):\r\n",
        "    print(join(dataset_path,target))\r\n",
        "    filenames.append(listdir(join(dataset_path, target)))\r\n",
        "    y.append(np.ones(len(filenames[index]))*index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlMs1IAO14b1"
      },
      "source": [
        "#정답벡터 y 검사 \r\n",
        "#정답벡터 y 검사 \r\n",
        "print(y)\r\n",
        "for item in y:\r\n",
        "    print(len(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP_Shumk1_k1"
      },
      "source": [
        "filenames = [item for sublist in filenames for item in sublist]\r\n",
        "y = [item for sublist in y for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUmo2ZGj2JXb"
      },
      "source": [
        "filenames_y=list(zip(filenames,y))\r\n",
        "random.shuffle(filenames_y)\r\n",
        "filenames,y=zip(*filenames_y)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hb8ZHQa3FZ1"
      },
      "source": [
        "print(len(filenames))\r\n",
        "filenames=filenames[:int(len(filenames)*perc_keep_samples)]\r\n",
        "print(len(filenames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5xQc6aZ3J_-"
      },
      "source": [
        "val_set_size =int(len(filenames)*val_ratio)\r\n",
        "test_set_size =int(len(filenames)*test_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhFBV8_R3PhD"
      },
      "source": [
        "\r\n",
        "filenames_val=filenames[:val_set_size]\r\n",
        "filenames_test=filenames[val_set_size:(val_set_size + test_set_size)]\r\n",
        "filenames_train=filenames[(val_set_size + test_set_size):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC-bwuqz3n-s"
      },
      "source": [
        "#y를 훈련, 유효 및 테스트 집합으로 분리 \r\n",
        "y_orig_val=y[:val_set_size]\r\n",
        "y_orig_test=y[val_set_size:(val_set_size + test_set_size)]\r\n",
        "y_orig_train=y[(val_set_size + test_set_size):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BwW1N2r3p1l"
      },
      "source": [
        "# 함수 MFCC 만들기 \r\n",
        "def calc_mfcc(path):\r\n",
        "    signal, fs = librosa.load(path,sr=sample_rate)\r\n",
        "    mfccs = python_speech_features.base.mfcc(signal,\r\n",
        "                                           samplerate=fs,\r\n",
        "                                           #winlen=0.256,\r\n",
        "                                           winlen=0.040,\r\n",
        "                                           #winstep=0.050,\r\n",
        "                                           winstep=0.020,\r\n",
        "                                           numcep=num_mfcc,\r\n",
        "                                           #nfilt=26,\r\n",
        "                                           #nfilt=40,\r\n",
        "                                           nfilt=20,\r\n",
        "                                           nfft=2048,\r\n",
        "                                           preemph=0.0,\r\n",
        "                                           ceplifter=0,\r\n",
        "                                           appendEnergy=False,\r\n",
        "                                           winfunc=np.hanning)\r\n",
        "    return mfccs.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lcd24iB3zEC"
      },
      "source": [
        "prob_cnt = 0\r\n",
        "x_test = []\r\n",
        "y_test = []\r\n",
        "for index, filename in enumerate(filenames_train):\r\n",
        "    #if index>=500:\r\n",
        "    #if index>=100:\r\n",
        "    #    break\r\n",
        "    path = join(dataset_path, target_list[int(y_orig_train[index])],\r\n",
        "               filename)\r\n",
        "    \r\n",
        "    mfccs =calc_mfcc(path)\r\n",
        "    \r\n",
        "    if mfccs.shape[1]==len_mfcc:\r\n",
        "        x_test.append(mfccs)\r\n",
        "        y_test.append(y_orig_train[index])\r\n",
        "    else:\r\n",
        "        print('Dropped:',index, mfccs.shape)\r\n",
        "        prob_cnt +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bXRxjvv70mM"
      },
      "source": [
        "print('% of problematic samples', prob_cnt/500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAIDbv5y759L"
      },
      "source": [
        "!pip install playsound\r\n",
        "from playsound import playsound\r\n",
        "idx=10\r\n",
        "path=join(dataset_path, target_list[int(y_orig_train[idx])],\r\n",
        "         filenames_train[idx])\r\n",
        "mfccs = calc_mfcc(path)\r\n",
        "print(\"MFCCs:\",mfccs)\r\n",
        "fig = plt.figure()\r\n",
        "plt.imshow(mfccs, cmap='inferno',origin='lower')\r\n",
        "print(target_list[int(y_orig_train[idx])])\r\n",
        "playsound(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNy9f8t-7-4P"
      },
      "source": [
        "def extract_features(in_files, in_y):\r\n",
        "    prob_cnt = 0\r\n",
        "    out_x = []\r\n",
        "    out_y = []\r\n",
        "    for index, filename in enumerate(in_files):\r\n",
        "        path = join(dataset_path,target_list[int(in_y[index])],\r\n",
        "                    filename)\r\n",
        "        if not path.endswith('.wav'):\r\n",
        "            continue\r\n",
        "            \r\n",
        "    \r\n",
        "        mfccs =calc_mfcc(path)\r\n",
        "    \r\n",
        "        if mfccs.shape[1]==len_mfcc:\r\n",
        "            out_x.append(mfccs)\r\n",
        "            out_y.append(in_y[index])\r\n",
        "        else:\r\n",
        "            print('Dropped:',index, mfccs.shape)\r\n",
        "            prob_cnt +=1\r\n",
        "    return out_x, out_y, prob_cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o8KAn3I73aO"
      },
      "source": [
        "x_train, y_train, prob = extract_features(filenames_train,\r\n",
        "                                         y_orig_train)\r\n",
        "print( 'Removed percentage:', prob/len(y_orig_train))\r\n",
        "x_val, y_val, prob = extract_features(filenames_val,\r\n",
        "                                         y_orig_val)\r\n",
        "print( 'Removed percentage:', prob/len(y_orig_val))\r\n",
        "x_test, y_test, prob = extract_features(filenames_test,\r\n",
        "                                         y_orig_test)\r\n",
        "print( 'Removed percentage:', prob/len(y_orig_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8GtUdwABy9F"
      },
      "source": [
        "np.savez(feature_sets_file,\r\n",
        "       x_train=x_train,\r\n",
        "       y_train=y_train,\r\n",
        "        x_val=x_val,\r\n",
        "        y_val=y_val,\r\n",
        "        x_test=x_test,\r\n",
        "        y_test=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzx2LGRcB2X0"
      },
      "source": [
        "feature_sets = np.load(feature_sets_file)\r\n",
        "feature_sets.files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWrbSFdBB5St"
      },
      "source": [
        "len(feature_sets['x_train'])\r\n",
        "len(feature_sets['y_train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb0WYS2_B7xE"
      },
      "source": [
        "print(feature_sets['y_val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh2OHaLJB_1e"
      },
      "source": [
        "len(feature_sets['x_val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWizAGjiCCRD"
      },
      "source": [
        "len(feature_sets['x_test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybyqg8R8LWkf"
      },
      "source": [
        "!wget https://github.com/ARM-software/ML-KWS-for-MCU/archive/master.zip\r\n",
        "!unzip master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ibBjH6zM2YY"
      },
      "source": [
        "feature_sets_path='/content'\r\n",
        "feature_sets_filename = 'all_targets_mfcc_sets_0123.npz'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W44dKlO1Ned5"
      },
      "source": [
        "feature_sets=np.load(join(feature_sets_path, feature_sets_filename))\r\n",
        "print(feature_sets.files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_1yJFCyC4RX"
      },
      "source": [
        "x_train=feature_sets['x_train']\r\n",
        "y_train=feature_sets['y_train']\r\n",
        "x_val=feature_sets['x_val']\r\n",
        "y_val=feature_sets['y_val']\r\n",
        "x_test=feature_sets['x_test']\r\n",
        "y_test=feature_sets['y_test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL_3YptiC5Ve"
      },
      "source": [
        "print(x_train.shape)\r\n",
        "print(x_val.shape)\r\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq8tKUt-C8F1"
      },
      "source": [
        "print(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k0dnVbhC_Bx"
      },
      "source": [
        "x_train=x_train.astype('float64')\r\n",
        "y_train=y_train.astype('float64')\r\n",
        "x_val=x_val.astype('float64')\r\n",
        "y_val=y_val.astype('float64')\r\n",
        "x_test=x_test.astype('float64')\r\n",
        "y_test=y_test.astype('float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aNRsstTDCtF"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  #tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
        "  tf.keras.layers.Flatten(input_shape=(20,49)),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dropout(0.2),\r\n",
        "  tf.keras.layers.Dense(38, activation='softmax')\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--hSN78QDFvr"
      },
      "source": [
        "model.compile(optimizer='adam',\r\n",
        "               loss='sparse_categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNVItThGOLjw"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx1Gl9PAOprE"
      },
      "source": [
        "model.evaluate(x_test, y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRW0Pp2fOyC_"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],\r\n",
        "                         x_train.shape[1],\r\n",
        "                         x_train.shape[2],\r\n",
        "                         1)\r\n",
        "x_val = x_val.reshape(x_val.shape[0],\r\n",
        "                         x_val.shape[1],\r\n",
        "                         x_val.shape[2],\r\n",
        "                         1)\r\n",
        "x_test = x_test.reshape(x_test.shape[0],\r\n",
        "                         x_test.shape[1],\r\n",
        "                         x_test.shape[2],\r\n",
        "                         1)\r\n",
        "print(x_train.shape)\r\n",
        "print(x_val.shape)\r\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiOlg5IOO7HW"
      },
      "source": [
        "#https://www.geeksforgeeks.org/python-image-classifiction-using keras/\r\n",
        "from tensorflow.keras import layers, models\r\n",
        "sample_shape=(20,49,1)\r\n",
        "model = models.Sequential()\r\n",
        "model.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=sample_shape, padding='same'))\r\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(layers.Conv2D(32, (2, 2), activation='relu',padding='same'))\r\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',padding='same'))\r\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "model.add(layers.Flatten())\r\n",
        "model.add(layers.Dense(64, activation='relu'))\r\n",
        "model.add(layers.Dropout(0.5))\r\n",
        "#model.add(layers.Dense(1, activation='softmax'))\r\n",
        "model.add(layers.Dense(38, activation='sigmoid'))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuKp5RnvQRjl"
      },
      "source": [
        "sample_shape=x_test.shape[1:]\r\n",
        "print(sample_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kukwda2QyBV"
      },
      "source": [
        "#https://www.geeksforgeeks.org/python-image-classifiction-using keras/\r\n",
        "#skip\r\n",
        "model = models.Sequential()\r\n",
        "model.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=sample_shape, padding='same'))\r\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(layers.Conv2D(32, (2, 2), activation='relu',padding='same'))\r\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu',padding='same'))\r\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "model.add(layers.Flatten())\r\n",
        "model.add(layers.Dense(64, activation='relu'))\r\n",
        "model.add(layers.Dropout(0.5))\r\n",
        "#model.add(layers.Dense(1, activation='softmax'))\r\n",
        "model.add(layers.Dense(38, activation='sigmoid'))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiMaESa9QZQ9"
      },
      "source": [
        "#model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam')\r\n",
        "model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy'], metrics=['sparse_categorical_accuracy'])\r\n",
        "#model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',learning_rate=0.01,metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi7XIqQYEORl"
      },
      "source": [
        "history=model.fit(x_train,y_train,epochs=30,batch_size=100,\r\n",
        "                 validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr1sL4CqShYA"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "#epochs=5\r\n",
        "acc=history.history['sparse_categorical_accuracy']\r\n",
        "val_acc=history.history['val_sparse_categorical_accuracy']\r\n",
        "loss=history.history['loss']\r\n",
        "val_loss=history.history['val_loss']\r\n",
        "\r\n",
        "epochs=range(1,len(acc)+1)\r\n",
        "plt.plot(epochs,acc,'bo', label='Training acc')\r\n",
        "plt.plot(epochs,val_acc,'b', label='Validation acc')\r\n",
        "plt.title('training and valodation accuracy')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "\r\n",
        "plt.plot(epochs,loss,'bo', label='Training loss')\r\n",
        "plt.plot(epochs,val_loss,'b', label='Validation loss')\r\n",
        "plt.title('training and valodation loss')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8epNOdLrXHql"
      },
      "source": [
        "import math\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\r\n",
        "def step_decay(epoch):\r\n",
        "    initial_lrate = 0.001\r\n",
        "    drop = 0.4\r\n",
        "    epochs_drop = 15.0\r\n",
        "    lrate = initial_lrate * math.pow(drop,  \r\n",
        "            math.floor((1+epoch)/epochs_drop))\r\n",
        "    \r\n",
        "    if (lrate < 4e-5):\r\n",
        "        lrate = 4e-5\r\n",
        "      \r\n",
        "    print('Changing learning rate to {}'.format(lrate))\r\n",
        "    return lrate\r\n",
        "lrate = LearningRateScheduler(step_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vz-HmfbXKMp"
      },
      "source": [
        "earlystopper = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=10,\r\n",
        "                             verbose=1, restore_best_weights=True)\r\n",
        "checkpointer = ModelCheckpoint('model-attRNN.h5', monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "results = model.fit(trainGen, validation_data=valGen, epochs=60, use_multiprocessing=False, workers=4, verbose=2,\r\n",
        "                    callbacks=[earlystopper, checkpointer, lrate])\r\n",
        "\r\n",
        "model.save('model-attRNN.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ19SVVyDYWN"
      },
      "source": [
        "# summarize history for categorical accuracy\r\n",
        "plt.plot(results.history['sparse_categorical_accuracy'])\r\n",
        "plt.plot(results.history['val_sparse_categorical_accuracy'])\r\n",
        "plt.title('Categorical accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "# summarize history for loss\r\n",
        "plt.plot(results.history['loss'])\r\n",
        "plt.plot(results.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MHulDEHDcQJ"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3JPF8h-DwdB"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "acc=history.history['acc']\r\n",
        "val_acc=history.history['val_acc']\r\n",
        "loss=history.history['loss']\r\n",
        "val_loss=history.history['val_loss']\r\n",
        "\r\n",
        "epochs=range(1,len(acc)+1)\r\n",
        "plt.plot(epochs,acc,'bo', label='Training acc')\r\n",
        "plt.plot(epochs,val_acc,'b', label='Validation acc')\r\n",
        "plt.title('training and valodation accuracy')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "plt.plot(epochs,loss,'bo', label='Training loss')\r\n",
        "plt.plot(epochs,val_loss,'b', label='Validation loss')\r\n",
        "plt.title('training and valodation loss')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}